{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Retrieval-Augmented Generation (RAG) combines the power of large language models (LLMs) with external knowledge sources to generate more accurate and contextually relevant responses. Here’s an overview of how to create a RAG model using a vector graph (for efficient retrieval) and a hybrid approach:\n\n1. Overview of RAG Hybrid Model:\n\nRetrieval Component: Retrieves relevant documents or snippets from a knowledge base using a vector graph or embeddings.\nAugmentation Component: The retrieved documents are used to augment the input to the language model, which generates a more informed response.\nGeneration Component: The language model (e.g., GPT-3, BERT) generates the final response, leveraging both the original query and the retrieved documents.\n\n2. Vector Graph for Efficient Retrieval:\n\nEmbedding Space: Documents or knowledge base entries are converted into embeddings using a pre-trained model like BERT, SBERT, or similar. These embeddings capture the semantic meaning of the text.\n\nVector Graph/Index: The embeddings are stored in a vector graph or a vector index (like FAISS or Annoy) for fast retrieval. This graph organizes the embeddings in a way that allows for efficient nearest neighbor search.\nRetrieval Process: Given a query, it is converted into an embedding, and the vector graph is queried to find the nearest neighbors (i.e., the most semantically similar documents).\n\n3. Hybrid RAG Model:\n\nQuery Embedding: The user query is first converted into an embedding.\nRetrieval: This embedding is used to query the vector graph to retrieve relevant documents.\nAugmentation: The retrieved documents are concatenated with the original query, forming an augmented input.\n\nGeneration: The augmented input is fed into the language model, which generates a response that integrates the retrieved information.\n\n4. Implementing RAG Hybrid Model:\n\nHere’s a basic implementation outline using Python with PyTorch and Hugging Face:","metadata":{}},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-08-16T00:43:47.658865Z","iopub.execute_input":"2024-08-16T00:43:47.659538Z","iopub.status.idle":"2024-08-16T00:44:00.113852Z","shell.execute_reply.started":"2024-08-16T00:43:47.659509Z","shell.execute_reply":"2024-08-16T00:44:00.112842Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: faiss-gpu in /opt/conda/lib/python3.10/site-packages (1.7.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer\nimport faiss\nimport numpy as np\n\n# Load the models\ntokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel_bert = BertModel.from_pretrained('bert-base-uncased')\ntokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\nmodel_gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Sample documents for vector indexing\ndocuments = [\n    \"info about skin cancer...\",\n    \"treatment options for skin cancer...\",\n    \"explaining symptoms of skin cancer...\"\n]\n\n# Convert documents to embeddings\nembeddings = []\nfor doc in documents:\n    inputs = tokenizer_bert(doc, return_tensors='pt', max_length=512, truncation=True)\n    with torch.no_grad():\n        embedding = model_bert(**inputs).pooler_output\n    embeddings.append(embedding.squeeze().numpy())\n\n# Create a vector index (FAISS)\ndimension = embeddings[0].shape[0]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(np.array(embeddings))\n\n# Function to get embeddings for a query\ndef get_query_embedding(query):\n    query_inputs = tokenizer_bert(query, return_tensors='pt', max_length=512, truncation=True)\n    with torch.no_grad():\n        query_embedding = model_bert(**query_inputs).pooler_output.squeeze().numpy()\n    return query_embedding\n\n# Retrieve top-k documents\ndef retrieve_documents(query, k=2):\n    query_embedding = get_query_embedding(query)\n    D, I = index.search(np.array([query_embedding]), k)\n    return [documents[i] for i in I[0]]\n\n# Generate a response using GPT-2\ndef generate_response(query, retrieved_docs):\n    augmented_query = query + \" \" + \" \".join(retrieved_docs)\n    inputs_gpt2 = tokenizer_gpt2(augmented_query, return_tensors='pt', max_length=512, truncation=True)\n    response = model_gpt2.generate(**inputs_gpt2)\n    return tokenizer_gpt2.decode(response[0], skip_special_tokens=True)\n\n# Example usage\nquery = \"What are the symptoms of skin cancer?\"\nretrieved_docs = retrieve_documents(query, k=2)\ngenerated_response = generate_response(query, retrieved_docs)\n\nprint(\"Query:\", query)\nprint(\"Retrieved Documents:\")\nfor doc in retrieved_docs:\n    print(f\"- {doc}\")\nprint(\"Generated Response:\", generated_response)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T00:44:00.116194Z","iopub.execute_input":"2024-08-16T00:44:00.116619Z","iopub.status.idle":"2024-08-16T00:44:01.863527Z","shell.execute_reply.started":"2024-08-16T00:44:00.116580Z","shell.execute_reply":"2024-08-16T00:44:01.862558Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Query: What are the symptoms of skin cancer?\nRetrieved Documents:\n- info about skin cancer...\n- treatment options for skin cancer...\nGenerated Response: What are the symptoms of skin cancer? info about skin cancer... treatment options for skin cancer...\n\n","output_type":"stream"}]}]}